{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for CLCLSA using BRCA as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete multi-omics classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from utils.data_utils import one_hot_tensor, prepare_trte_data, get_mask\n",
    "from networks.models.clcl import CLUECL3\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of subjects = 875, training = 612, testing = 263\n",
      "number of omics type = 3\n",
      "shape of each omics in training data:\n",
      "shape of 0-th omics training data = torch.Size([612, 1000])\n",
      "shape of 1-th omics training data = torch.Size([612, 503])\n",
      "shape of 2-th omics training data = torch.Size([612, 1000])\n",
      "shape of each omics in testing data:\n",
      "shape of 0-th omics testing data= torch.Size([263, 1000])\n",
      "shape of 1-th omics testing data= torch.Size([263, 503])\n",
      "shape of 2-th omics testing data= torch.Size([263, 1000])\n",
      "data type = <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"BRCA\"\n",
    "# training data, testing data, training and testing indices, labels\n",
    "data_tr_list, data_test_list, trte_idx, labels_trte = prepare_trte_data(data_folder, True)\n",
    "print(f\"total number of subjects = {len(labels_trte)}, training = {data_tr_list[0].shape[0]}, testing = {data_test_list[0].shape[0]}\")\n",
    "print(f\"number of omics type = {len(data_tr_list)}\")\n",
    "print(f\"shape of each omics in training data:\")\n",
    "for i in range(len(data_tr_list)):\n",
    "    print(f\"shape of {i}-th omics training data = {data_tr_list[i].shape}\")\n",
    "print(f\"shape of each omics in testing data:\")\n",
    "for i in range(len(data_tr_list)):\n",
    "    print(f\"shape of {i}-th omics testing data= {data_test_list[i].shape}\")\n",
    "print(f\"data type = {type(data_tr_list[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing data\n",
    "num_class = len(np.unique(labels_trte))       # number of class in the multi-omics dataset\n",
    "labels_tr_tensor = torch.LongTensor(labels_trte[trte_idx[\"tr\"]])\n",
    "onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)\n",
    "labels_tr_tensor = labels_tr_tensor.cuda()\n",
    "onehot_labels_tr_tensor = onehot_labels_tr_tensor.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build moodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_list = [x.shape[1] for x in data_tr_list] # dimension of each omics data\n",
    "hidden_dim = \"200\"                            # number of units in the hidden_layer\n",
    "hidden_dim = [int(x) for x in hidden_dim.split(\",\")]\n",
    "dropout = 0.5                                 # dropout rate\n",
    "cross_omics_prediction_layers = \"64,32\"       # hyper parameters for cross-omics latent feature impuatation\n",
    "prediction = {i: [int(x) for x in cross_omics_prediction_layers.split(\",\")] for i in range(3)}\n",
    "model = CLUECL3(dim_list, hidden_dim, num_class, dropout, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and move model to GPU\n",
    "learning_rate = 1e-4                # learning rate for optimization\n",
    "step_size = 500                     # step size for the scheduler\n",
    "num_epoch = 2500                    # number of total training epochs\n",
    "test_inverval = 500                 # number of epoch for testing\n",
    "missing_rate = 0.                   # set 0 for complete multi-omics setting\n",
    "device = \"cuda\"                     # use cuda for GPU training\n",
    "lambda_al = 0.1                     # balancing factor for auxilary classifier\n",
    "lambda_co = 0.1                     # balancing factor for cross-omics encoder\n",
    "lambda_cil = 0.1                    # balancing factor for comtrastive learning\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training one epoch\n",
    "def train_epoch(model, optimizer, missing_rate, data_tr_list, labels_tr_tensor, print, device, lambda_al, lambda_co, lambda_cil, mask_train=None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if missing_rate > 0:\n",
    "        loss, _, loss_dict = model.train_missing_cg(data_tr_list, mask_train, labels_tr_tensor, device,\n",
    "                                                    aux_loss=lambda_al>0, lambda_al=lambda_al,\n",
    "                                                    cross_omics_loss=lambda_co>0, lambda_col=lambda_co,\n",
    "                                                    constrastive_instance_loss=lambda_cil>0, lambda_cil=lambda_cil,\n",
    "                                                    constrastive_cluster_loss=False, lambda_ccl=0.)\n",
    "    else:\n",
    "        loss, _, loss_dict = model(data_tr_list, labels_tr_tensor, aux_loss=lambda_al>0, lambda_al=lambda_al)\n",
    "    \n",
    "    if print:\n",
    "        pprint.pprint(loss_dict)\n",
    "    loss = torch.mean(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# function for testing one epoch\n",
    "def test_epoch(model, missing_rate, data_test_list, device, mask_test=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if missing_rate > 0:\n",
    "            logit = model.infer_on_missing(data_test_list, mask_test, device)\n",
    "        else:\n",
    "            logit = model.infer(data_test_list)\n",
    "        prob = F.softmax(logit, dim=1).data.cpu().numpy()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2501/2501 [00:43<00:00, 56.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_acc = 0.870722433460076, best_acc = 0.870722433460076, best_weighted_f1 = 0.8727098387935339, best_macro_f1 = 0.8358228702640293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global_acc = 0.\n",
    "best_eval = []\n",
    "best_epoch = 0\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in tqdm(range(num_epoch + 1)):\n",
    "    # print_loss = True if epoch % test_inverval == 0 else False\n",
    "    print_loss = False\n",
    "    # def train_epoch(model, optimizer, missing_rate, data_tr_list, labels_tr_tensor, print, device, lambda_al, lambda_co, lambda_cil, mask_train=None):\n",
    "    train_epoch(model, optimizer, missing_rate, data_tr_list, labels_tr_tensor, print_loss, device, lambda_al, lambda_co, lambda_cil)\n",
    "\n",
    "    scheduler.step()\n",
    "    if epoch % test_inverval == 0:\n",
    "        # def test_epoch(model, missing_rate, data_test_list, device, mask_test=None):\n",
    "        te_prob = test_epoch(model, missing_rate, data_test_list, device)\n",
    "        if not np.any(np.isnan(te_prob)):\n",
    "            # print(\"\\nTest: Epoch {:d}\".format(epoch))\n",
    "            if num_class == 2:\n",
    "                acc = accuracy_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n",
    "                f1 = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n",
    "                auc = roc_auc_score(labels_trte[trte_idx[\"te\"]], te_prob[:, 1])\n",
    "                # print(f\"Test ACC: {acc:.5f}, F1: {f1:.5f}, AUC: {auc:.5f}\")\n",
    "                if acc > global_acc:\n",
    "                    global_acc, best_epoch = acc, epoch\n",
    "                    best_eval = [acc, f1, auc]\n",
    "            else:\n",
    "                acc = accuracy_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n",
    "                f1w = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='weighted')\n",
    "                f1m = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='macro')\n",
    "                # print(f\"Test ACC: {acc:.5f}, F1 weighted : {f1w:.5f}, F1 macro: {f1m:.5f}\")\n",
    "                if acc > global_acc:\n",
    "                    global_acc = acc\n",
    "                    best_eval, best_epoch = [acc, f1w, f1m], epoch\n",
    "print(f\"global_acc = {global_acc}, best_acc = {best_eval[0]}, best_weighted_f1 = {best_eval[1]}, best_macro_f1 = {best_eval[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cmomplete multi-omics classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of subjects = 875, training = 612, testing = 263\n",
      "number of omics type = 3\n",
      "shape of each omics in training data:\n",
      "shape of 0-th omics training data = torch.Size([612, 1000])\n",
      "shape of 1-th omics training data = torch.Size([612, 503])\n",
      "shape of 2-th omics training data = torch.Size([612, 1000])\n",
      "shape of each omics in testing data:\n",
      "shape of 0-th omics testing data= torch.Size([263, 1000])\n",
      "shape of 1-th omics testing data= torch.Size([263, 503])\n",
      "shape of 2-th omics testing data= torch.Size([263, 1000])\n",
      "data type = <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"BRCA\"\n",
    "# training data, testing data, training and testing indices, labels\n",
    "data_tr_list, data_test_list, trte_idx, labels_trte = prepare_trte_data(data_folder, True)\n",
    "print(f\"total number of subjects = {len(labels_trte)}, training = {data_tr_list[0].shape[0]}, testing = {data_test_list[0].shape[0]}\")\n",
    "print(f\"number of omics type = {len(data_tr_list)}\")\n",
    "print(f\"shape of each omics in training data:\")\n",
    "for i in range(len(data_tr_list)):\n",
    "    print(f\"shape of {i}-th omics training data = {data_tr_list[i].shape}\")\n",
    "print(f\"shape of each omics in testing data:\")\n",
    "for i in range(len(data_tr_list)):\n",
    "    print(f\"shape of {i}-th omics testing data= {data_test_list[i].shape}\")\n",
    "print(f\"data type = {type(data_tr_list[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing data\n",
    "num_class = len(np.unique(labels_trte))       # number of class in the multi-omics dataset\n",
    "labels_tr_tensor = torch.LongTensor(labels_trte[trte_idx[\"tr\"]])\n",
    "onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)\n",
    "labels_tr_tensor = labels_tr_tensor.cuda()\n",
    "onehot_labels_tr_tensor = onehot_labels_tr_tensor.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate in-complete multi-omics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_wrapper(n_views, data_len, missing_rate):\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            mask = get_mask(n_views, data_len, missing_rate)\n",
    "            success = True\n",
    "        except:\n",
    "            success = False\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate = 0.1 # set missing rate\n",
    "mask = get_mask(3, data_tr_list[0].shape[0], missing_rate)\n",
    "mask = torch.from_numpy(np.asarray(mask, dtype=np.float32)).to(device)\n",
    "x1_train = data_tr_list[0] * torch.unsqueeze(mask[:, 0], 1)\n",
    "x2_train = data_tr_list[1] * torch.unsqueeze(mask[:, 1], 1)\n",
    "x3_train = data_tr_list[2] * torch.unsqueeze(mask[:, 2], 1)\n",
    "mask_train = mask\n",
    "data_tr_list = [x1_train, x2_train, x3_train]\n",
    "mask = get_mask_wrapper(3, data_test_list[0].shape[0], missing_rate)\n",
    "mask = torch.from_numpy(np.asarray(mask, dtype=np.float32)).to(device)\n",
    "x1_test = data_test_list[0] * torch.unsqueeze(mask[:, 0], 1)\n",
    "x2_test = data_test_list[1] * torch.unsqueeze(mask[:, 1], 1)\n",
    "x3_test = data_test_list[2] * torch.unsqueeze(mask[:, 2], 1)\n",
    "mask_test = mask\n",
    "data_test_list = [x1_test, x2_test, x3_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build moodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_list = [x.shape[1] for x in data_tr_list] # dimension of each omics data\n",
    "hidden_dim = \"200\"                            # number of units in the hidden_layer\n",
    "hidden_dim = [int(x) for x in hidden_dim.split(\",\")]\n",
    "dropout = 0.5                                 # dropout rate\n",
    "cross_omics_prediction_layers = \"64,32\"       # hyper parameters for cross-omics latent feature impuatation\n",
    "prediction = {i: [int(x) for x in cross_omics_prediction_layers.split(\",\")] for i in range(3)}\n",
    "model = CLUECL3(dim_list, hidden_dim, num_class, dropout, prediction)\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Traning and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4                # learning rate for optimization\n",
    "step_size = 500                     # step size for the scheduler\n",
    "num_epoch = 2500                    # number of total training epochs\n",
    "test_inverval = 500                 # number of epoch for testing\n",
    "missing_rate = 0.                   # set 0 for complete multi-omics setting\n",
    "device = \"cuda\"                     # use cuda for GPU training\n",
    "lambda_al = 0.1                     # balancing factor for auxilary classifier\n",
    "lambda_co = 0.1                     # balancing factor for cross-omics encoder\n",
    "lambda_cil = 0.1                    # balancing factor for comtrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2501/2501 [00:42<00:00, 59.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_acc = 0.8631178707224335, best_acc = 0.8631178707224335, best_weighted_f1 = 0.8672270264074836, best_macro_f1 = 0.822689924546818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global_acc = 0.\n",
    "best_eval = []\n",
    "best_epoch = 0\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in tqdm(range(num_epoch + 1)):\n",
    "    # print_loss = True if epoch % test_inverval == 0 else False\n",
    "    print_loss = False\n",
    "    # def train_epoch(model, optimizer, missing_rate, data_tr_list, labels_tr_tensor, print, device, lambda_al, lambda_co, lambda_cil, mask_train=None):\n",
    "    train_epoch(model, optimizer, missing_rate, data_tr_list, labels_tr_tensor, print_loss, device, lambda_al, lambda_co, lambda_cil, mask_train)\n",
    "\n",
    "    scheduler.step()\n",
    "    if epoch % test_inverval == 0:\n",
    "        # def test_epoch(model, missing_rate, data_test_list, device, mask_test=None):\n",
    "        te_prob = test_epoch(model, missing_rate, data_test_list, device, mask_test)\n",
    "        if not np.any(np.isnan(te_prob)):\n",
    "            # print(\"\\nTest: Epoch {:d}\".format(epoch))\n",
    "            if num_class == 2:\n",
    "                acc = accuracy_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n",
    "                f1 = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n",
    "                auc = roc_auc_score(labels_trte[trte_idx[\"te\"]], te_prob[:, 1])\n",
    "                # print(f\"Test ACC: {acc:.5f}, F1: {f1:.5f}, AUC: {auc:.5f}\")\n",
    "                if acc > global_acc:\n",
    "                    global_acc, best_epoch = acc, epoch\n",
    "                    best_eval = [acc, f1, auc]\n",
    "            else:\n",
    "                acc = accuracy_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n",
    "                f1w = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='weighted')\n",
    "                f1m = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='macro')\n",
    "                # print(f\"Test ACC: {acc:.5f}, F1 weighted : {f1w:.5f}, F1 macro: {f1m:.5f}\")\n",
    "                if acc > global_acc:\n",
    "                    global_acc = acc\n",
    "                    best_eval, best_epoch = [acc, f1w, f1m], epoch\n",
    "print(f\"global_acc = {global_acc}, best_acc = {best_eval[0]}, best_weighted_f1 = {best_eval[1]}, best_macro_f1 = {best_eval[2]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
